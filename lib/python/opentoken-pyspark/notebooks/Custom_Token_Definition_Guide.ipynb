{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Token Definition Guide\n",
    "\n",
    "This notebook demonstrates how to create custom token definitions with minimal code using the OpenToken notebook helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for token generation\n",
    "HASHING_SECRET = \"my-hashing-secret-key\"\n",
    "ENCRYPTION_KEY = \"my-encryption-key-32-characters!\"\n",
    "\n",
    "from opentoken_pyspark.notebook_helpers import (\n",
    "    TokenBuilder,\n",
    "    CustomTokenDefinition,\n",
    "    create_token_generator,\n",
    "    quick_token,\n",
    "    list_attributes,\n",
    "    expression_help\n",
    ")\n",
    "from opentoken.attributes.general.record_id_attribute import RecordIdAttribute\n",
    "from opentoken.attributes.person.first_name_attribute import FirstNameAttribute\n",
    "from opentoken.attributes.person.last_name_attribute import LastNameAttribute\n",
    "from opentoken.attributes.person.birth_date_attribute import BirthDateAttribute\n",
    "from opentoken.attributes.person.sex_attribute import SexAttribute\n",
    "from opentoken.attributes.person.social_security_number_attribute import SocialSecurityNumberAttribute\n",
    "from opentoken.attributes.person.postal_code_attribute import PostalCodeAttribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Available Attributes\n",
    "\n",
    "Check what attributes are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = list_attributes()\n",
    "print(\"Available attributes:\")\n",
    "for name in attrs.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression Syntax Help\n",
    "\n",
    "Get help on expression syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expression_help())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Quick Token (Simplest)\n",
    "\n",
    "Create a custom T6 token in one line with the rule:\n",
    "`U(last-name)|U(first-name)|birth-date|postal-code-3|U(sex)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create T6 token generator in one call\n",
    "generator = quick_token(\n",
    "    \"T6\",\n",
    "    [\n",
    "        (\"last_name\", \"T|U\"),\n",
    "        (\"first_name\", \"T|U\"),\n",
    "        (\"birth_date\", \"T|D\"),\n",
    "        (\"postal_code\", \"T|S(0,3)\"),\n",
    "        (\"sex\", \"T|U\")\n",
    "    ],\n",
    "    HASHING_SECRET,\n",
    "    ENCRYPTION_KEY\n",
    ")\n",
    "\n",
    "# Test it with sample data\n",
    "person_attrs = {\n",
    "    RecordIdAttribute: \"1\",\n",
    "    FirstNameAttribute: \"John\",\n",
    "    LastNameAttribute: \"Doe\",\n",
    "    BirthDateAttribute: \"1990-01-15\",\n",
    "    SexAttribute: \"Male\",\n",
    "    PostalCodeAttribute: \"98101\"\n",
    "}\n",
    "\n",
    "result = generator.get_all_tokens(person_attrs)\n",
    "print(f\"T6 Token: {result.tokens.get('T6')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Token Builder (More Flexible)\n",
    "\n",
    "Use the fluent TokenBuilder API for more control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom T6 token\n",
    "t6_token = TokenBuilder(\"T6\") \\\n",
    "    .add(\"last_name\", \"T|U\") \\\n",
    "    .add(\"first_name\", \"T|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .add(\"postal_code\", \"T|S(0,3)\") \\\n",
    "    .add(\"sex\", \"T|U\") \\\n",
    "    .build()\n",
    "\n",
    "# Create a custom token definition\n",
    "custom_definition = CustomTokenDefinition().add_token(t6_token)\n",
    "\n",
    "# Create token generator\n",
    "generator = create_token_generator(\n",
    "    HASHING_SECRET,\n",
    "    ENCRYPTION_KEY,\n",
    "    custom_definition\n",
    ")\n",
    "\n",
    "# Generate tokens\n",
    "result = generator.get_all_tokens(person_attrs)\n",
    "print(f\"T6 Token: {result.tokens.get('T6')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Multiple Custom Tokens\n",
    "\n",
    "Define multiple tokens in one definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create T6 token: U(last-name)|U(first-name)|birth-date|postal-code-3|U(sex)\n",
    "t6_token = TokenBuilder(\"T6\") \\\n",
    "    .add(\"last_name\", \"T|U\") \\\n",
    "    .add(\"first_name\", \"T|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .add(\"postal_code\", \"T|S(0,3)\") \\\n",
    "    .add(\"sex\", \"T|U\") \\\n",
    "    .build()\n",
    "\n",
    "# Create T7 token: U(last-name-3)|U(first-name-3)|birth-date\n",
    "t7_token = TokenBuilder(\"T7\") \\\n",
    "    .add(\"last_name\", \"T|S(0,3)|U\") \\\n",
    "    .add(\"first_name\", \"T|S(0,3)|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .build()\n",
    "\n",
    "# Add both tokens to definition\n",
    "custom_definition = CustomTokenDefinition() \\\n",
    "    .add_token(t6_token) \\\n",
    "    .add_token(t7_token)\n",
    "\n",
    "# Create generator\n",
    "generator = create_token_generator(\n",
    "    HASHING_SECRET,\n",
    "    ENCRYPTION_KEY,\n",
    "    custom_definition\n",
    ")\n",
    "\n",
    "# Generate both tokens\n",
    "result = generator.get_all_tokens(person_attrs)\n",
    "print(f\"T6 Token: {result.tokens.get('T6')}\")\n",
    "print(f\"T7 Token: {result.tokens.get('T7')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom Tokens with PySpark DataFrames\n",
    "\n",
    "The key is to pass your custom TokenDefinition to the OpenTokenProcessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from opentoken_pyspark import OpenTokenProcessor\n",
    "from opentoken_pyspark.notebook_helpers import TokenBuilder, CustomTokenDefinition\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomTokens\").getOrCreate()\n",
    "\n",
    "# Create sample DataFrame with ALL required columns (even if not used by custom token)\n",
    "# Note: Currently OpenTokenProcessor validates ALL standard columns regardless of custom tokens\n",
    "data = [\n",
    "    (\"1\", \"John\", \"Doe\", \"1990-01-15\", \"Male\", \"98101\", \"123-45-6789\"),\n",
    "    (\"2\", \"Jane\", \"Smith\", \"1985-06-20\", \"Female\", \"94105\", \"987-65-4321\")\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"RecordId\", \"FirstName\", \"LastName\", \"BirthDate\", \"Sex\", \"PostalCode\", \"SocialSecurityNumber\"])\n",
    "\n",
    "# Step 1: Define your custom T6 token\n",
    "t6_token = TokenBuilder(\"T6\") \\\n",
    "    .add(\"last_name\", \"T|U\") \\\n",
    "    .add(\"first_name\", \"T|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .add(\"postal_code\", \"T|S(0,3)\") \\\n",
    "    .add(\"sex\", \"T|U\") \\\n",
    "    .build()\n",
    "\n",
    "# Step 2: Create custom token definition\n",
    "custom_definition = CustomTokenDefinition().add_token(t6_token)\n",
    "\n",
    "# Step 3: Create processor with custom definition\n",
    "processor = OpenTokenProcessor(\n",
    "    hashing_secret=HASHING_SECRET,\n",
    "    encryption_key=ENCRYPTION_KEY,\n",
    "    token_definition=custom_definition\n",
    ")\n",
    "\n",
    "# Step 4: Process DataFrame with custom tokens\n",
    "tokens_df = processor.process_dataframe(df)\n",
    "\n",
    "# Show results - you'll see T6 tokens instead of T1-T5!\n",
    "print(\"Custom T6 tokens generated:\")\n",
    "print(\"DataFrame structure:\")\n",
    "tokens_df.printSchema()\n",
    "tokens_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Custom Tokens with PySpark\n",
    "\n",
    "You can also use multiple custom tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple custom tokens\n",
    "t6_token = TokenBuilder(\"T6\") \\\n",
    "    .add(\"last_name\", \"T|U\") \\\n",
    "    .add(\"first_name\", \"T|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .add(\"postal_code\", \"T|S(0,3)\") \\\n",
    "    .add(\"sex\", \"T|U\") \\\n",
    "    .build()\n",
    "\n",
    "t7_token = TokenBuilder(\"T7\") \\\n",
    "    .add(\"last_name\", \"T|S(0,3)|U\") \\\n",
    "    .add(\"first_name\", \"T|S(0,3)|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .build()\n",
    "\n",
    "# Add both to definition\n",
    "multi_definition = CustomTokenDefinition() \\\n",
    "    .add_token(t6_token) \\\n",
    "    .add_token(t7_token)\n",
    "\n",
    "# Create processor with multiple custom tokens\n",
    "processor_multi = OpenTokenProcessor(\n",
    "    hashing_secret=HASHING_SECRET,\n",
    "    encryption_key=ENCRYPTION_KEY,\n",
    "    token_definition=multi_definition\n",
    ")\n",
    "\n",
    "# Process - will generate both T6 and T7 tokens\n",
    "multi_tokens_df = processor_multi.process_dataframe(df)\n",
    "\n",
    "print(\"Multiple custom tokens (T6 and T7):\")\n",
    "multi_tokens_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Rules\n",
    "\n",
    "Try different combinations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Processing Options\n",
    "\n",
    "OpenTokenProcessor supports three modes for token generation:\n",
    "\n",
    "1. **Hash + Encrypt** (Production): Most secure, tokens are hashed then encrypted\n",
    "2. **Hash Only** (Experimentation): Faster, tokens are hashed but not encrypted  \n",
    "3. **Plain Text** (Debugging): Raw concatenated strings like `DOE|JOHN|1990-01-15|981|MALE`\n",
    "\n",
    "Simply omit the `hashing_secret` and/or `encryption_key` to control the mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the modules to pick up the latest changes\n",
    "import importlib\n",
    "import opentoken_pyspark.token_processor\n",
    "import opentoken.tokens.token_generator\n",
    "importlib.reload(opentoken.tokens.token_generator)\n",
    "importlib.reload(opentoken_pyspark.token_processor)\n",
    "from opentoken_pyspark.token_processor import OpenTokenProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Hash + Encrypt (most secure - production use)\n",
    "processor_encrypted = OpenTokenProcessor(\n",
    "    hashing_secret=HASHING_SECRET,\n",
    "    encryption_key=ENCRYPTION_KEY,\n",
    "    token_definition=multi_definition\n",
    ")\n",
    "\n",
    "# Option 2: Hash only (no encryption - faster for experimentation)\n",
    "processor_hashed = OpenTokenProcessor(\n",
    "    hashing_secret=HASHING_SECRET,\n",
    "    token_definition=multi_definition\n",
    ")\n",
    "\n",
    "# Option 3: Plain text (no hash, no encryption - debugging only)  \n",
    "processor_plain = OpenTokenProcessor(\n",
    "    token_definition=multi_definition\n",
    ")\n",
    "\n",
    "# Process with all three modes\n",
    "encrypted_df = processor_encrypted.process_dataframe(df)\n",
    "hashed_df = processor_hashed.process_dataframe(df)\n",
    "plain_df = processor_plain.process_dataframe(df)\n",
    "\n",
    "print(\"=== Encrypted Tokens (Hash + Encrypt) ===\")\n",
    "encrypted_df.show(truncate=False)\n",
    "\n",
    "print(\"\\n=== Hashed Tokens (Hash only) ===\")\n",
    "hashed_df.show(truncate=False)\n",
    "\n",
    "print(\"\\n=== Plain Tokens (Raw concatenated strings) ===\")\n",
    "print(\"Example: DOE|JOHN|1990-01-15|981|MALE\")\n",
    "plain_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal token: just last name and first initial\n",
    "minimal_token = TokenBuilder(\"MINIMAL\") \\\n",
    "    .add(\"last_name\", \"T|U\") \\\n",
    "    .add(\"first_name\", \"T|S(0,1)|U\") \\\n",
    "    .build()\n",
    "\n",
    "# Full token: everything (needs SSN added to person_attrs)\n",
    "full_token = TokenBuilder(\"FULL\") \\\n",
    "    .add(\"last_name\", \"T|U\") \\\n",
    "    .add(\"first_name\", \"T|U\") \\\n",
    "    .add(\"birth_date\", \"T|D\") \\\n",
    "    .add(\"sex\", \"T|U\") \\\n",
    "    .add(\"postal_code\", \"T|S(0,5)\") \\\n",
    "    .add(\"ssn\", \"T\") \\\n",
    "    .build()\n",
    "\n",
    "# Create definition with both\n",
    "definition = CustomTokenDefinition() \\\n",
    "    .add_token(minimal_token) \\\n",
    "    .add_token(full_token)\n",
    "\n",
    "generator = create_token_generator(\n",
    "    HASHING_SECRET,\n",
    "    ENCRYPTION_KEY,\n",
    "    definition\n",
    ")\n",
    "\n",
    "person_attrs_with_ssn = person_attrs.copy()\n",
    "person_attrs_with_ssn[SocialSecurityNumberAttribute] = \"234-56-7890\"  # Valid format\n",
    "\n",
    "result = generator.get_all_tokens(person_attrs_with_ssn)\n",
    "print(f\"Minimal Token: {result.tokens.get('MINIMAL')}\")\n",
    "print(f\"Full Token: {result.tokens.get('FULL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Three ways to create custom tokens:\n",
    "\n",
    "1. **`quick_token()`** - Fastest, one-liner approach for simple cases\n",
    "2. **`TokenBuilder`** - Fluent API for readable, flexible definitions\n",
    "3. **Manual classes** - Full control (as shown in previous examples)\n",
    "\n",
    "Choose the method that best fits your workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
