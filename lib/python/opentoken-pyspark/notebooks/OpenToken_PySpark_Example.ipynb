{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenToken PySpark Example\n",
    "\n",
    "This notebook demonstrates how to use the OpenToken PySpark bridge to generate privacy-preserving tokens from a PySpark DataFrame.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Install the required packages:\n",
    "   ```bash\n",
    "   cd lib/python\n",
    "   pip install -e .\n",
    "   cd ../python-pyspark\n",
    "   pip install -e .\n",
    "   ```\n",
    "\n",
    "2. Ensure you have PySpark and Jupyter installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from opentoken_pyspark import OpenTokenProcessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenTokenExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Data\n",
    "\n",
    "We'll load the sample CSV data into a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data from CSV\n",
    "sample_csv_path = \"../../../../resources/sample.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    sample_csv_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Display the schema\n",
    "print(\"Input DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenToken Processor\n",
    "\n",
    "Create an instance of the OpenTokenProcessor with your hashing secret and encryption key.\n",
    "\n",
    "**Note:** The secrets used here are for demonstration purposes only. In production, use secure secrets management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor with secrets\n",
    "processor = OpenTokenProcessor(\n",
    "    hashing_secret=\"HashingKey\",\n",
    "    encryption_key=\"Secret-Encryption-Key-Goes-Here.\"\n",
    ")\n",
    "\n",
    "print(\"OpenToken Processor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tokens\n",
    "\n",
    "Process the DataFrame to generate tokens for each person record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tokens\n",
    "tokens_df = processor.process_dataframe(df)\n",
    "\n",
    "# Display the schema of the result\n",
    "print(\"Output DataFrame Schema:\")\n",
    "tokens_df.printSchema()\n",
    "\n",
    "# Count total tokens generated\n",
    "total_tokens = tokens_df.count()\n",
    "print(f\"\\nTotal tokens generated: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "\n",
    "Let's look at the generated tokens for a specific record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tokens for the first record\n",
    "first_record_id = df.select(\"RecordId\").first()[0]\n",
    "print(f\"Tokens for RecordId: {first_record_id}\")\n",
    "\n",
    "tokens_df.filter(tokens_df.RecordId == first_record_id).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Token Distribution\n",
    "\n",
    "Check how many tokens were generated per rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens by RuleId\n",
    "print(\"Token count by RuleId:\")\n",
    "tokens_df.groupBy(\"RuleId\").count().orderBy(\"RuleId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Pandas for Visualization\n",
    "\n",
    "For smaller datasets, you can convert to Pandas for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a subset to Pandas for visualization\n",
    "sample_tokens = tokens_df.limit(10).toPandas()\n",
    "print(\"Sample tokens as Pandas DataFrame:\")\n",
    "display(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save the tokens to a Parquet file for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet\n",
    "output_path = \"../output/tokens_output.parquet\"\n",
    "\n",
    "tokens_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Tokens saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using Alternative Column Names\n",
    "\n",
    "OpenToken supports alternative column names for flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with alternative column names\n",
    "alt_data = [\n",
    "    {\n",
    "        \"Id\": \"custom-001\",\n",
    "        \"GivenName\": \"Alice\",\n",
    "        \"Surname\": \"Johnson\",\n",
    "        \"ZipCode\": \"98052\",\n",
    "        \"Gender\": \"Female\",\n",
    "        \"DateOfBirth\": \"1990-05-15\",\n",
    "        \"NationalIdentificationNumber\": \"234-56-7890\"\n",
    "    }\n",
    "]\n",
    "\n",
    "alt_df = spark.createDataFrame(alt_data)\n",
    "\n",
    "# Process with alternative column names\n",
    "alt_tokens_df = processor.process_dataframe(alt_df)\n",
    "\n",
    "print(\"Tokens generated with alternative column names:\")\n",
    "alt_tokens_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "For large datasets, PySpark processes data in parallel across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions\n",
    "print(f\"Number of partitions in input DataFrame: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"Number of partitions in output DataFrame: {tokens_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop the Spark session when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "print(\"Session complete. Uncomment the line above to stop Spark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. Loading person data into a PySpark DataFrame\n",
    "2. Initializing the OpenToken processor with secrets\n",
    "3. Generating privacy-preserving tokens for each record\n",
    "4. Analyzing and visualizing the results\n",
    "5. Saving tokens for further use\n",
    "\n",
    "The PySpark bridge enables distributed token generation for large-scale person matching workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
